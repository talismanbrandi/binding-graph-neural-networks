{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3416b067-bc64-4d23-99bb-c686eb1793be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import uuid\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import argparse\n",
    "import pytorch_model_summary as pms\n",
    "import pyreadr as pyr\n",
    "\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('text', usetex=True)\n",
    "plt.rcParams['text.latex.preamble'] = ''\n",
    "\n",
    "CLIP = 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25459a76-117e-4a3a-884b-4f9eb7b40859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    ''' a function to load the data\n",
    "        argument:\n",
    "            config: the configuration file for the run containing the pat to the data source\n",
    "        returns:\n",
    "            df_int: uint8 encoded input data\n",
    "    '''\n",
    "    logging.info(' loading data from file {}'.format(config['data_dir']))\n",
    "    \n",
    "    # input files\n",
    "    config['data_dir'] = config['data_dir']+'/' if config['data_dir'][-1] != '/' else config['data_dir']\n",
    "    config['data_fname'] = config['data_dir']+config['cell_line']+ '-'+config['exon_buffer']+'-50-50/regression_'+config['cell_line'] +'_binding_psi_psip_KD.rds'\n",
    "    config['rbp_list'] = config['data_dir']+config['cell_line']+ '-'+config['exon_buffer']+'-50-50/rbp_'+config['cell_line'] +'.rds'\n",
    "    \n",
    "    df = pd.DataFrame(pyr.read_r(config['data_fname'])[None])\n",
    "    config['n_rbp'] = pd.DataFrame(pyr.read_r(config['rbp_list'])[None]).shape[0] * 6\n",
    "    config['n_samples'] = {}\n",
    "    config['n_samples']['total'] = df.shape[0]\n",
    "    \n",
    "    if config['annotate_cell_lines']:\n",
    "        if 'cell_line' not in df.columns:\n",
    "                logging.error(' cell line annotation not possible as cell line is not made explicit in the data set')\n",
    "                sys.exit()\n",
    "        else:\n",
    "            unique_cl = df.cell_line.unique()\n",
    "            for cl in unique_cl:\n",
    "                config['n_samples'][cl] = {}\n",
    "                config['n_samples'][cl]['n_samples'] = df[df.cell_line == cl].shape[0]\n",
    "                config['n_samples'][cl]['fraction'] = df[df.cell_line == cl].shape[0]/config['n_samples']['total']\n",
    "    \n",
    "    if config['sample_size'] == 0 or config['sample_size'] > df.shape[0]:\n",
    "        df = df.sample(frac=1)\n",
    "    else:\n",
    "        df = df.sample(n=config['sample_size'])\n",
    "        \n",
    "    df_int = df.iloc[:,:config['n_rbp']].astype('uint8').copy()\n",
    "    df = pd.concat([df_int, df.iloc[:,config['n_rbp']:]], axis=1, copy=False).copy()\n",
    "    del df_int\n",
    "    \n",
    "    df_control = df[df.RBP_KD=='NONE'].copy()\n",
    "    df_control['target'] = df_control.psi\n",
    "    df_knockdown = df[df.RBP_KD!='NONE'].copy()\n",
    "    df_knockdown['target'] = df_knockdown.psip\n",
    "    df = pd.concat([df_control, df_knockdown], copy=False).sample(frac=1).copy()\n",
    "    \n",
    "    if config['annotate_cell_lines']:\n",
    "        map_cell = {unique_cl[i]: i for i in range(len(unique_cl))}\n",
    "        df['id_cell_line'] = df.cell_line.apply(lambda x: map_cell[x]).astype('uint8')\n",
    "        df = pd.concat([df.iloc[:,:config['n_rbp']], df[['id_cell_line', 'target']]], axis=1, copy=False)\n",
    "    else:\n",
    "        df = pd.concat([df.iloc[:,:config['n_rbp']], df['target']], axis=1, copy=False)\n",
    "    \n",
    "    del df_control\n",
    "    del df_knockdown\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "class df_to_tensor(torch.utils.data.Dataset):\n",
    "    ''' class to convert dataframe to torch tensor\n",
    "    '''\n",
    " \n",
    "    def __init__(self, df, config):\n",
    "        self.df = df.copy(deep = True)\n",
    "\n",
    "        # define variables and target\n",
    "        var_y = config['var_y']\n",
    "        x = df.drop(config['var_y'], axis=1).values\n",
    "\n",
    "        if config['scaled'] == 'normal':\n",
    "            normalize(df, config)\n",
    "            y = df['y_scaled'].values\n",
    "        elif config['scaled'] == 'log':\n",
    "            df['y_log_scaled'] = np.log(df[var_y])\n",
    "            normalize(df, config, var_y='y_log_scaled')\n",
    "            y = df['y_scaled'].values\n",
    "        else:\n",
    "            y = df[var_y].values\n",
    "            config[\"mu\"] = 0\n",
    "            config[\"sigma\"] = 1\n",
    "\n",
    "\n",
    "        self.x = torch.tensor(x, dtype=torch.float64).to(get_device())\n",
    "        self.y = torch.tensor(y, dtype=torch.float64).reshape(-1, 1).to(get_device())\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "    \n",
    "    \n",
    "def build_data_loaders(df, config):\n",
    "    ''' function to build the data loaders\n",
    "        arguments:\n",
    "            df: pandas dataframe\n",
    "            config: configuration file\n",
    "        returns:\n",
    "            train_loader: the training set loader\n",
    "            validation_loader: the validation set loader\n",
    "            test_loader: the test set loader\n",
    "    '''\n",
    "    \n",
    "    # slice and dice the data into train, validation and test sets\n",
    "    df_test_data =  df.sample(frac=config[\"test_split\"], random_state=config['seed'])\n",
    "    df_validation_data = df.drop(df_test_data.index).sample(frac=config[\"validation_split\"], random_state=config['seed'])\n",
    "    df_train_data = df.drop(df_test_data.index).drop(df_validation_data.index).sample(frac=1, random_state=config['seed'])\n",
    "\n",
    "    test_data = df_to_tensor(df_test_data, config)\n",
    "    validation_data = df_to_tensor(df_validation_data, config)\n",
    "    train_data = df_to_tensor(df_train_data, config)\n",
    "\n",
    "    # load the data into torch tensor batches\n",
    "    batch = config[\"batch_size\"]\n",
    "    numworkers = 2 if get_device().type == 'cpu' else 0\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch, shuffle=True, num_workers=numworkers)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch, shuffle=True, num_workers=numworkers)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch, shuffle=False, num_workers=numworkers)\n",
    "    \n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "\n",
    "#############################\n",
    "# ML Stuff\n",
    "#############################\n",
    "\n",
    "def get_device():\n",
    "    ''' function to get the device the NN is running on, CPU or GPU\n",
    "    '''\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    else: \n",
    "        device = torch.device(\"cpu\")\n",
    "        \n",
    "    return device\n",
    "\n",
    "def init_torch(config):\n",
    "    ''' function for initializing the seeds\n",
    "    '''\n",
    "    gen = torch.manual_seed(config['seed'])\n",
    "    \n",
    "    device = get_device()\n",
    "\n",
    "    if device.type == 'cpu':\n",
    "        torch.set_num_threads(64)\n",
    "        # torch.set_num_interop_threads(1)\n",
    "\n",
    "    logging.info(f' torch is using {device}')\n",
    "    \n",
    "    return device\n",
    "\n",
    "class EarlyStopping:\n",
    "    ''' class for early stopping with patience\n",
    "    '''\n",
    "    def __init__(self, m_path, patience=1, min_delta=0):\n",
    "        '''\n",
    "            arguments:\n",
    "                m_path: the model path where the checkpoints go\n",
    "                patience: the number of epochs the patience lasts\n",
    "                min_delta: the minimum change that is monitored\n",
    "        '''\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "        self.m_path = m_path\n",
    "        \n",
    "    def reset_counter(self):\n",
    "        ''' function to reset the counter\n",
    "        '''\n",
    "        self.counter = 0\n",
    "\n",
    "    def early_stop(self, model, validation_loss):\n",
    "        ''' function to check for the early stopping threshold\n",
    "            arguments:\n",
    "                model: the torch model\n",
    "                validation_loss: the validation loss\n",
    "        '''\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            torch.save(model.state_dict(), self.m_path)\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "class skip_block(torch.nn.Module):\n",
    "    \"\"\" the basic building block of a dnn with skip connections\n",
    "        arguments:\n",
    "            x: the input\n",
    "            width: the width of the hidden layers\n",
    "            activation: the activation function: 'relu', 'elu', 'swish' (silu), 'leaky_relu', 'softplus'\n",
    "            squeeze: a boolean specifying wheher the skip units are squeezed\n",
    "        returns:\n",
    "            res: the skip net block\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, width, activation, stream = False, n_layers=1):\n",
    "        super(skip_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.width = width\n",
    "        self.stream = stream\n",
    "        \n",
    "        self.input = torch.nn.Linear(self.input_shape, self.width)\n",
    "        self.fc_module = torch.nn.ModuleList([torch.nn.Linear(self.width, self.width) for i in range(n_layers)])\n",
    "        \n",
    "        if self.stream:\n",
    "            self.linear = torch.nn.Linear(self.width, self.input_shape)\n",
    "        else:    \n",
    "            self.linear = torch.nn.Linear(self.width, self.width)\n",
    "            \n",
    "        if self.input_shape != self.width:\n",
    "            self.reshape = torch.nn.Linear(self.input_shape, self.width, bias=False)\n",
    "        \n",
    "        self.act = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.act(self.input(x))\n",
    "        for l in self.fc_module:\n",
    "            y = self.act(l(y))\n",
    "        if self.stream:\n",
    "            y = self.act(self.linear(y))\n",
    "            y += x\n",
    "            return y\n",
    "        else:\n",
    "            y = self.linear(y)\n",
    "            if self.input_shape != self.width:\n",
    "                residual = self.reshape(x)\n",
    "            else:\n",
    "                residual = x\n",
    "            y += residual\n",
    "\n",
    "            return self.act(y)\n",
    "        \n",
    "    \n",
    "    \n",
    "class skip_dnn(torch.nn.Module):\n",
    "    ''' class for the DNN with skip connections: see https://arxiv.org/abs/2302.00753\n",
    "    '''\n",
    "    def __init__(self, sk_block, config, stream = False):\n",
    "        super(skip_dnn, self).__init__()\n",
    "        self.width = config[\"width\"]\n",
    "        self.n_blocks = config[\"depth\"] - 1\n",
    "        self.input_shape = config[\"input_shape\"]\n",
    "        self.stream = stream\n",
    "        \n",
    "        if config[\"activation\"] == 'leaky_relu':\n",
    "            self.act = torch.nn.LeakyReLU()\n",
    "        elif config[\"activation\"] == 'relu':\n",
    "            self.act = torch.nn.ReLU()\n",
    "        if config[\"activation\"] == 'softplus':\n",
    "            self.act = torch.nn.Softplus()\n",
    "        if config[\"activation\"] == 'swish':\n",
    "            self.act = torch.nn.SiLU()\n",
    "        \n",
    "        self.input = skip_block(self.input_shape, self.width, self.act, stream=self.stream)\n",
    "        self.core = self.make_layers(skip_block)\n",
    "        if self.stream: \n",
    "            self.output = torch.nn.Linear(self.input_shape, 1)\n",
    "        else: \n",
    "            self.output = torch.nn.Linear(self.width, 1)\n",
    "            \n",
    "    def make_layers(self, skip_block):\n",
    "        layers = []\n",
    "        for bl in range(self.n_blocks):\n",
    "            if self.stream: \n",
    "                layers.append(skip_block(self.input_shape, self.width, self.act, stream=self.stream))\n",
    "            else:\n",
    "                layers.append(skip_block(self.width, self.width, self.act))\n",
    "            \n",
    "        return torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.core(x)\n",
    "        return self.output(x)\n",
    "    \n",
    "    \n",
    "class dnn(torch.nn.Module):\n",
    "    ''' class for a feed-forward DNN\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(dnn, self).__init__()\n",
    "        self.width = config[\"width\"]\n",
    "        \n",
    "        self.input = torch.nn.Linear(config[\"input_shape\"], self.width)\n",
    "        self.fc_module = torch.nn.ModuleList([torch.nn.Linear(self.width, self.width) for i in range(config[\"depth\"] - 1)])\n",
    "        self.output = torch.nn.Linear(self.width, 1)\n",
    "        \n",
    "        if config[\"activation\"] == 'leaky_relu':\n",
    "            self.act = torch.nn.LeakyReLU()\n",
    "        elif config[\"activation\"] == 'relu':\n",
    "            self.act = torch.nn.ReLU()\n",
    "        if config[\"activation\"] == 'softplus':\n",
    "            self.act = torch.nn.Softplus()\n",
    "        if config[\"activation\"] == 'swish':\n",
    "            self.act = torch.nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.input(x))\n",
    "        for l in self.fc_module:\n",
    "            x = self.act(l(x))\n",
    "        return self.output(x)\n",
    "    \n",
    "    \n",
    "def nets(config):\n",
    "    \"\"\" the pytorch model builder\n",
    "        arguments:\n",
    "            config: the configuration file\n",
    "        returns:\n",
    "            regressor: the pytorch model\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the torch model\n",
    "    if config[\"model_type\"] == 'dnn':\n",
    "        regressor = dnn(config).double().to(get_device())\n",
    "    elif config[\"model_type\"] == 'skip':\n",
    "        regressor = skip_dnn(skip_block, config).double().to(get_device())\n",
    "    elif config[\"model_type\"] == 'skip-stream':\n",
    "        regressor = skip_dnn(skip_block, config, stream = True).double().to(get_device())\n",
    "    else:\n",
    "        logging.error(' '+config[\"model_type\"]+' not implemented. model_type can be either dnn, skip or squeeze')\n",
    "        \n",
    "        \n",
    "    # save parameter counts\n",
    "    summary = pms.summary(regressor, torch.zeros((config[\"input_shape\"],)).to(get_device()).double().clone().detach().requires_grad_(True)).rstrip().split('\\n')\n",
    "    config[\"trainable_parameters\"] = int(summary[-3].replace(',', '')[18:])\n",
    "    config[\"non_trainable_parameters\"] = int(summary[-2].replace(',', '')[22:])\n",
    "    config[\"total_parameters\"] = int(summary[-4].replace(',', '')[14:])\n",
    "    \n",
    "    # save config\n",
    "    with open(config['directory']+'/config-'+config['model-uuid']+'.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "        \n",
    "    return regressor\n",
    "\n",
    "\n",
    "def lp_loss(p, model):\n",
    "    ''' function for p-regularization. p = 1 lasso, p = 2 ridge\n",
    "        argument:\n",
    "            model: the torch model\n",
    "    '''\n",
    "    lp_regularization = torch.tensor(0., requires_grad=True)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:\n",
    "            lp_regularization = lp_regularization + torch.norm(param, p=p)\n",
    "    return lp_regularization\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_data, f_optimizer, f_loss, max_steps, alpha = 0, beta = 0):\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_data):\n",
    "        \n",
    "        # steps per epoch\n",
    "        if i == max_steps:\n",
    "            break\n",
    "        \n",
    "        # forward prop\n",
    "        x, y = data\n",
    "        f_optimizer.zero_grad()\n",
    "        y_out = model(x)\n",
    "\n",
    "        # backprop\n",
    "        loss = f_loss(y_out, y)\n",
    "        loss = loss + beta * (alpha * lp_loss(1, model) + (1 - alpha) * lp_loss(2, model))\n",
    "        if loss.item() > CLIP:\n",
    "            return loss.item()\n",
    "        loss.backward()\n",
    "        f_optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss/(i + 1)\n",
    "\n",
    "def validate_one_epoch(model, validate_data, f_loss, max_steps, alpha = 0, beta = 0):\n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    for i, data in enumerate(validate_data):\n",
    "        if i == max_steps:\n",
    "            break\n",
    "        \n",
    "        # forward prop\n",
    "        x, y = data\n",
    "        y_p = model(x)\n",
    "        \n",
    "        # loss\n",
    "        loss = f_loss(y_p, y)\n",
    "        loss = loss + beta * (alpha * lp_loss(1, model) + (1 - alpha) * lp_loss(2, model))\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if i == 0:\n",
    "            y_test = y.cpu().detach().numpy()\n",
    "            y_pred = y_p.cpu().detach().numpy()\n",
    "        else: \n",
    "            y_test = np.vstack((y_test, y.cpu().detach().numpy()))\n",
    "            y_pred = np.vstack((y_pred, y_p.cpu().detach().numpy()))\n",
    "        \n",
    "    # accuracy\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    y_pred = np.array(y_pred).reshape(-1,1)\n",
    "    abs_score = 0\n",
    "    r2_score = metrics.r2_score(y_test, y_pred)*100\n",
    "\n",
    "    return epoch_loss / (i + 1), abs_score, r2_score\n",
    "\n",
    "\n",
    "def test_model(model, test_data, config):\n",
    "    \n",
    "    for i, data in enumerate(test_data):\n",
    "        \n",
    "        # forward prop\n",
    "        x, y = data\n",
    "        y_p = model(x)\n",
    "        \n",
    "        # store data\n",
    "        if i == 0:\n",
    "            x_test = x.cpu().detach().numpy()\n",
    "            y_test = y.cpu().detach().numpy()\n",
    "            y_pred = y_p.cpu().detach().numpy()\n",
    "        else: \n",
    "            x_test = np.vstack((x_test, x.cpu().detach().numpy()))\n",
    "            y_test = np.vstack((y_test, y.cpu().detach().numpy()))\n",
    "            y_pred = np.vstack((y_pred, y_p.cpu().detach().numpy()))\n",
    "        \n",
    "    \n",
    "    # accuracy\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    y_pred = np.array(y_pred).reshape(-1,1)\n",
    "    abs_score = 0 #(1 - np.mean(np.abs((y_pred - y_test)/y_test)))*100\n",
    "    r2_score = metrics.r2_score(y_test, y_pred)*100\n",
    "    \n",
    "    # save test results\n",
    "    df_pred = pd.DataFrame(x_test)\n",
    "    df_pred.columns = list(config['RBP_list'].values())\n",
    "    df_pred['y_test'] = y_test\n",
    "    df_pred['y_pred'] = y_pred\n",
    "    df_pred['delta'] = np.array((y_pred - y_test)/y_test*100).reshape(-1,1)\n",
    "    df_pred.to_csv(config['directory']+'/test-results-'+config['model-uuid']+'-'+f'-{abs_score:.6f}-{r2_score:.6f}.csv')\n",
    "    \n",
    "    return abs_score, r2_score\n",
    "\n",
    "\n",
    "def runML(df, config):\n",
    "    \"\"\" run the DNN\n",
    "        arguments:\n",
    "            df: the dataframe including, training, validation and test\n",
    "            config: the configuration dictionary for the hyperparameters\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(' running the regressor')\n",
    "    train_loader, validation_loader, test_loader = build_data_loaders(df,config)\n",
    "    config['input_shape'] = df.shape[1] - 1 \n",
    "    config['RBP_list'] = {str(i): val for i, val in enumerate(df.columns[:-1])}\n",
    "    \n",
    "    regressor = nets(config)\n",
    "\n",
    "    # print the summary\n",
    "    logging.info('\\n' + pms.summary(regressor, torch.zeros((config[\"input_shape\"],)).to(get_device()).double().clone().detach().requires_grad_(True)))\n",
    "    \n",
    "    # define the loss function\n",
    "    if config['loss'] == 'mse':\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "    elif config['loss'] == 'mae':\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "    else:\n",
    "        raise ValueError('loss type not defined. Has to be mae or mse')\n",
    "            \n",
    "    # define the optimizer\n",
    "    optimizer = torch.optim.Adam(regressor.parameters(), lr=0.001)\n",
    "    \n",
    "    # learning rate decay\n",
    "    if config['lr_decay_type'] == 'exp':\n",
    "        gamma = (0.5 ** (10000000 / config[\"decay_steps\"])) ** (1 / 2500)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                        optimizer=optimizer,\n",
    "                        gamma=gamma\n",
    "                    )\n",
    "    elif config['lr_decay_type'] == 'poly':\n",
    "        scheduler = torch.optim.lr_scheduler.PolynomialLR(\n",
    "                        optimizer=optimizer,\n",
    "                        total_iters=config['decay_steps'],\n",
    "                        power=0.5\n",
    "                    )\n",
    "    elif config['lr_decay_type'] == 'const':\n",
    "        scheduler = torch.optim.lr_scheduler.ConstantLR(\n",
    "                        optimizer=optimizer,\n",
    "                        factor=1., \n",
    "                        total_iters=config[\"epochs\"]\n",
    "                    )\n",
    "    else:\n",
    "        raise ValueError('lr type not defined. Has to be exp or poly or const')\n",
    "        \n",
    "    # the training history\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"lr\": [], \"abs_score\": [], \"r2_score\": []}\n",
    "    \n",
    "    # model storage\n",
    "    model_path = config['directory'] + f'/dnn-{config[\"depth\"]}-{config[\"width\"]}-{config[\"activation\"]}-adam-{config[\"lr_decay_type\"]}-schedule-{config[\"loss\"]}-{config[\"monitor\"]}.torch'\n",
    "    \n",
    "    # early stopping\n",
    "    early_stopping = EarlyStopping(model_path, patience=config[\"patience\"])\n",
    "    \n",
    "    \n",
    "    # run the regressor\n",
    "    start = time.time()\n",
    "    epoch = 0\n",
    "    while epoch < config[\"epochs\"]:\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        regressor.train(True)\n",
    "        avg_loss = train_one_epoch(regressor, \n",
    "                                   train_loader, \n",
    "                                   optimizer, \n",
    "                                   loss_fn, \n",
    "                                   config[\"steps_per_epoch\"],\n",
    "                                   config[\"alpha\"], \n",
    "                                   config[\"beta\"])\n",
    "        \n",
    "        # for some activations there are spikes in the loss function: skip the epoch when that happens\n",
    "        if avg_loss > CLIP:\n",
    "            regressor.train(False)\n",
    "            early_stopping.reset_counter()\n",
    "            regressor.load_state_dict(torch.load(model_path))\n",
    "            continue\n",
    "            \n",
    "        # add to history\n",
    "        history[\"epoch\"].append(epoch + 1)\n",
    "        history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "\n",
    "        # We don't need gradients on to do reporting\n",
    "        regressor.train(False)\n",
    "        avg_vloss, abs_score, r2_score = validate_one_epoch(regressor, \n",
    "                                                            validation_loader, \n",
    "                                                            loss_fn, \n",
    "                                                            config[\"steps_per_epoch\"],\n",
    "                                                            config[\"alpha\"], \n",
    "                                                            config[\"beta\"])\n",
    "        history[\"val_loss\"].append(avg_vloss)\n",
    "        history[\"abs_score\"].append(abs_score)\n",
    "        history[\"r2_score\"].append(r2_score)\n",
    "        \n",
    "        logging.info(f' Epoch {epoch + 1}: training loss = {avg_loss:.8f}  validation loss = {avg_vloss:.8f}  learning rate = {optimizer.param_groups[0][\"lr\"]:0.3e}  relative accuracy: {abs_score:.2f}  R2 score: {r2_score:.2f}')\n",
    "        \n",
    "        # check for early stopping\n",
    "        if early_stopping.early_stop(regressor, avg_vloss):\n",
    "            regressor.load_state_dict(torch.load(model_path))\n",
    "            break\n",
    "\n",
    "        # if early stopping did not happen revert to the best model\n",
    "        if epoch == config[\"epochs\"] - 1:\n",
    "            regressor.load_state_dict(torch.load(model_path))\n",
    "            \n",
    "        # decay learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch += 1\n",
    "        \n",
    "    config[\"fit_time\"] = timediff(time.time() - start)\n",
    "    \n",
    "    return test_loader, regressor, history\n",
    "\n",
    "\n",
    "#############################\n",
    "# utils\n",
    "#############################\n",
    "\n",
    "def timediff(x):\n",
    "    \"\"\" a function to convert seconds to hh:mm:ss\n",
    "        argument:\n",
    "            x: time in seconds\n",
    "        returns:\n",
    "            time in hh:mm:ss\n",
    "    \"\"\"\n",
    "    return \"{}:{}:{}\".format(int(x/3600), str(int(x/60%60)).zfill(2), str(round(x - int(x/3600)*3600 - int(x/60%60)*60)).zfill(2))\n",
    "\n",
    "def plot_loss(history, dir_name):\n",
    "    \"\"\" plotting routine\n",
    "        argument:\n",
    "            history: the tf history object\n",
    "    \"\"\"\n",
    "    plt.plot(history['train_loss'], label='loss')\n",
    "    plt.plot(history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('y')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(dir_name+'/training-evaluation.pdf', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "def labeler(config) -> str:\n",
    "    return '{}-{} ({:,})'.format(config['depth'], config['width'], config['trainable_parameters'])\n",
    "\n",
    "def make_error_plot(config, df):\n",
    "    lim = df['delta'].std()*5\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.hist(df[abs(df['delta']) < lim]['delta'], bins=200, histtype='step', linewidth=3, density=True, label=labeler(config))\n",
    "    plt.xlabel(r'$\\delta$ (\\%)', fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xlim((-lim, lim))\n",
    "    plt.grid(linestyle='dashed', alpha=0.4, color='#808080')\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.title(r'$\\delta$ distribution [mean: {:.4f}%, std: {:.4f}%]'.format(df['delta'].mean(), df['delta'].std()), fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config['directory']+'/delta-distribution.pdf', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def post_process(regressor, test_loader, history, config):\n",
    "    \"\"\" post process the regressor to check for accuracy and save everything\n",
    "        argumants:\n",
    "            regressor: the tensorflow regressor object\n",
    "            history: the history object for the training\n",
    "            config: the configuration for the training\n",
    "    \"\"\"\n",
    "    logging.info(' running post-process')\n",
    "    \n",
    "    # check accuracy\n",
    "    logging.info(' running the DNN predictions and accuracy computation')\n",
    "    \n",
    "    abs_score, r2_score = test_model(regressor, test_loader, config)\n",
    "    config[\"abs_score\"] = abs_score\n",
    "    config[\"r2_score\"] = r2_score\n",
    "    logging.info(' relative accuracy: {:.2f}%  |---|  R2 score: {:.2f}%'.format(abs_score, r2_score))\n",
    "        \n",
    "    #plot the training history\n",
    "    logging.info(' printing training evaluation plots')\n",
    "    plot_loss(history, config['directory'])\n",
    "    \n",
    "    # end time\n",
    "    config[\"end_time\"] = time.strftime(\"%Y-%m-%d %H:%M:%S %z\", time.localtime())\n",
    "                 \n",
    "    # save config\n",
    "    with open(config['directory']+'/config-'+config['model-uuid']+'.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "        \n",
    "    # save history\n",
    "    with open(config['directory']+'/history-'+config['model-uuid']+'.json', 'w') as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "        \n",
    "    # remove preliminary config file\n",
    "    os.remove(config['directory']+'/config-'+config['model-uuid']+'-prelim.json')\n",
    "        \n",
    "    # move directory\n",
    "    shutil.move(config['directory'], config['directory']+'-'+config['scaled']+'-'+str(config['depth'])+'-'+str(config['width'])+'-'+config['activation']+'-'+str(config['batch_size'])+'-adam-'+config['lr_decay_type']+'-schedule-'+config['loss']+'-'+config['monitor']+f'-{abs_score:.6f}-{r2_score:.6f}') \n",
    "    \n",
    "    \n",
    "#############################\n",
    "# main\n",
    "#############################\n",
    "    \n",
    "def main(df):\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    with open('config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    #init torch\n",
    "    device = init_torch(config)\n",
    "    \n",
    "    # start time\n",
    "    config[\"start_time\"] = time.strftime(\"%Y-%m-%d %H:%M:%S %z\", time.localtime())\n",
    "    \n",
    "    # set device\n",
    "    config[\"device\"] = device.type + ':' + device.index if device.index else device.type\n",
    "    \n",
    "    #  create directory structure\n",
    "    if config['model-uuid'] == \"UUID\":\n",
    "        m_uuid = str(uuid.uuid4())[:8]\n",
    "        config['model-uuid'] = m_uuid\n",
    "    else:\n",
    "        m_uuid = config['model-uuid']\n",
    "        \n",
    "    if config['base_directory'] != '':\n",
    "        base_directory = config['base_directory'] +'/' if config['base_directory'][-1] != '/' else config['base_directory']\n",
    "        dir_name = base_directory+config['model_type']+'-torch-'+config['cell_line']+'-'+str(config['var_y'])+'-'+m_uuid\n",
    "    else:\n",
    "        dir_name = config['model_type']+'-torch-'+config['cell_line']+'-'+str(config['var_y'])+'-'+m_uuid\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    config['directory'] = dir_name\n",
    "    \n",
    "    # save the config\n",
    "    with open(config['directory']+'/config-'+config['model-uuid']+'-prelim.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    # train the regressor\n",
    "    test_loader, regressor, history = runML(df, config)\n",
    "\n",
    "    # post process the results and save everything\n",
    "    post_process(regressor, test_loader, history, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0cc79-808b-4734-b893-dab25c89440d",
   "metadata": {},
   "source": [
    "_______________\n",
    "### Prepare the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66a3ce-bd8a-4a94-a1c4-f49b0c6cbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_type\": 'skip',              # the architecture of the base model. Can be skip for sk-DNN or dnn for regular DNN\n",
    "    \"cell_line\": 'HepG2',              # the cell \n",
    "    \"exon_buffer\": \"100\",              # the exon buffer. Its the characteristic of the data\n",
    "    \"data_dir\": \"../data/\",            # the directory where the data is\n",
    "    \"seed\": 42,                        # seed is always 42\n",
    "    \"var_y\": \"target\",                 # the column header for the target distribution\n",
    "    \"activation\": 'relu',              # the acitvation function being used\n",
    "    \"width\": 30,                       # the width of the network\n",
    "    \"depth\": 10,                       # the depth of the network. For sk-DNN it is the numbe of blocks.\n",
    "    \"beta\" : 0.001,                    # the weight of the ridge regressions\n",
    "    \"alpha\": 0.,                       # the weight of the lasso regression\n",
    "    \"scaled\": \"none\",                  # scaling of the data, normally not done\n",
    "    \"lr_decay_type\": \"exp\",            # learning rate decay function. Can be exp, poly, or const\n",
    "    \"initial_lr\": 0.001,               # initial learning rate\n",
    "    \"final_lr\": 1e-06,                 # final learning rate\n",
    "    \"decay_steps\": 500000,             # decayse steps of the learning rate\n",
    "    \"validation_split\": 0.2,           # validation split used for early stopping\n",
    "    \"test_split\": 0.2,                 # test split used to test the final model\n",
    "    \"batch_size\": 256,                 # batch size for stochastic gradient descent\n",
    "    \"steps_per_epoch\": 200,            # gradient descent steps per epoch. Early stopping criterion is checked at the end of an epoch\n",
    "    \"patience\": 50,                    # the patience for early stopping in unites of epochs\n",
    "    \"monitor\": \"val_mse\",              # the error monitored for early stopping\n",
    "    \"loss\": \"mse\",                     # the loss function. Usually mse.\n",
    "    'sample_size': 5000000,            # the total smaple size used for training\n",
    "    \"verbose\": 1,                      # verbosity\n",
    "    \"base_directory\": \"../models/\",    # the directory in which the final model will be stored\n",
    "    \"epochs\": 2000,                    # the maximum number of epochs\n",
    "    \"model-uuid\": \"UUID\",              # the model UUID to make the run unique\n",
    "    \"annotate_cell_lines\": False       # annotation of cell lines in the input data. Keep to False for now.\n",
    "}\n",
    "\n",
    "with open('config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd82b4e-bb98-44c3-ac00-9e55982648df",
   "metadata": {},
   "source": [
    "_________\n",
    "### Load data\n",
    "This takes a whle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d706b4b-3349-41be-afac-3baa1a1246fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "loadData = False\n",
    "if loadData:\n",
    "    df = load_data(config)\n",
    "    loadData = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d6a71-16c7-4f56-8503-da1de56f85ae",
   "metadata": {},
   "source": [
    "_______________________\n",
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeccb9d-91de-426e-81a2-bd76a610f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: torch is using cpu\n",
      "INFO:root: running the regressor\n",
      "INFO:root:\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "      skip_block-1                [30]          31,770          31,770\n",
      "      skip_block-2                [30]           2,790           2,790\n",
      "      skip_block-3                [30]           2,790           2,790\n",
      "      skip_block-4                [30]           2,790           2,790\n",
      "      skip_block-5                [30]           2,790           2,790\n",
      "      skip_block-6                [30]           2,790           2,790\n",
      "      skip_block-7                [30]           2,790           2,790\n",
      "      skip_block-8                [30]           2,790           2,790\n",
      "      skip_block-9                [30]           2,790           2,790\n",
      "     skip_block-10                [30]           2,790           2,790\n",
      "         Linear-11                 [1]              31              31\n",
      "=======================================================================\n",
      "Total params: 56,911\n",
      "Trainable params: 56,911\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "INFO:root: Epoch 1: training loss = 0.16678555  validation loss = 0.13284556  learning rate = 1.000e-03  relative accuracy: 0.00  R2 score: 21.91\n",
      "INFO:root: Epoch 2: training loss = 0.12609005  validation loss = 0.11902946  learning rate = 9.461e-04  relative accuracy: 0.00  R2 score: 26.50\n",
      "INFO:root: Epoch 3: training loss = 0.11613428  validation loss = 0.11129216  learning rate = 8.950e-04  relative accuracy: 0.00  R2 score: 28.01\n",
      "INFO:root: Epoch 4: training loss = 0.10690368  validation loss = 0.10264300  learning rate = 8.467e-04  relative accuracy: 0.00  R2 score: 33.83\n",
      "INFO:root: Epoch 5: training loss = 0.10086592  validation loss = 0.09863859  learning rate = 8.011e-04  relative accuracy: 0.00  R2 score: 33.26\n",
      "INFO:root: Epoch 6: training loss = 0.09510281  validation loss = 0.09381531  learning rate = 7.579e-04  relative accuracy: 0.00  R2 score: 35.68\n",
      "INFO:root: Epoch 7: training loss = 0.09089916  validation loss = 0.09167696  learning rate = 7.170e-04  relative accuracy: 0.00  R2 score: 34.72\n",
      "INFO:root: Epoch 8: training loss = 0.08807683  validation loss = 0.08712078  learning rate = 6.783e-04  relative accuracy: 0.00  R2 score: 39.15\n",
      "INFO:root: Epoch 9: training loss = 0.08489171  validation loss = 0.08248444  learning rate = 6.417e-04  relative accuracy: 0.00  R2 score: 41.31\n",
      "INFO:root: Epoch 10: training loss = 0.08209691  validation loss = 0.07928563  learning rate = 6.071e-04  relative accuracy: 0.00  R2 score: 42.14\n",
      "INFO:root: Epoch 11: training loss = 0.08018070  validation loss = 0.07840087  learning rate = 5.743e-04  relative accuracy: 0.00  R2 score: 41.81\n",
      "INFO:root: Epoch 12: training loss = 0.07744810  validation loss = 0.07628955  learning rate = 5.434e-04  relative accuracy: 0.00  R2 score: 43.58\n",
      "INFO:root: Epoch 13: training loss = 0.07521167  validation loss = 0.07376670  learning rate = 5.141e-04  relative accuracy: 0.00  R2 score: 44.94\n",
      "INFO:root: Epoch 14: training loss = 0.07445781  validation loss = 0.07410906  learning rate = 4.863e-04  relative accuracy: 0.00  R2 score: 43.84\n",
      "INFO:root: Epoch 15: training loss = 0.07240505  validation loss = 0.07319769  learning rate = 4.601e-04  relative accuracy: 0.00  R2 score: 43.34\n",
      "INFO:root: Epoch 16: training loss = 0.07132883  validation loss = 0.07196173  learning rate = 4.353e-04  relative accuracy: 0.00  R2 score: 43.97\n",
      "INFO:root: Epoch 17: training loss = 0.07121631  validation loss = 0.06954995  learning rate = 4.118e-04  relative accuracy: 0.00  R2 score: 45.74\n",
      "INFO:root: Epoch 18: training loss = 0.06891111  validation loss = 0.07003677  learning rate = 3.896e-04  relative accuracy: 0.00  R2 score: 45.16\n",
      "INFO:root: Epoch 19: training loss = 0.06883910  validation loss = 0.06912847  learning rate = 3.686e-04  relative accuracy: 0.00  R2 score: 46.26\n",
      "INFO:root: Epoch 20: training loss = 0.06794230  validation loss = 0.06843071  learning rate = 3.487e-04  relative accuracy: 0.00  R2 score: 46.15\n",
      "INFO:root: Epoch 21: training loss = 0.06764778  validation loss = 0.06710465  learning rate = 3.299e-04  relative accuracy: 0.00  R2 score: 47.54\n",
      "INFO:root: Epoch 22: training loss = 0.06629716  validation loss = 0.06648791  learning rate = 3.121e-04  relative accuracy: 0.00  R2 score: 47.85\n",
      "INFO:root: Epoch 23: training loss = 0.06595081  validation loss = 0.06544013  learning rate = 2.952e-04  relative accuracy: 0.00  R2 score: 48.69\n",
      "INFO:root: Epoch 24: training loss = 0.06624526  validation loss = 0.06548186  learning rate = 2.793e-04  relative accuracy: 0.00  R2 score: 49.30\n",
      "INFO:root: Epoch 25: training loss = 0.06545141  validation loss = 0.06612570  learning rate = 2.643e-04  relative accuracy: 0.00  R2 score: 48.12\n",
      "INFO:root: Epoch 26: training loss = 0.06507825  validation loss = 0.06467424  learning rate = 2.500e-04  relative accuracy: 0.00  R2 score: 49.21\n",
      "INFO:root: Epoch 27: training loss = 0.06524875  validation loss = 0.06529233  learning rate = 2.365e-04  relative accuracy: 0.00  R2 score: 47.97\n",
      "INFO:root: Epoch 28: training loss = 0.06536720  validation loss = 0.06550279  learning rate = 2.238e-04  relative accuracy: 0.00  R2 score: 48.52\n",
      "INFO:root: Epoch 29: training loss = 0.06412659  validation loss = 0.06426933  learning rate = 2.117e-04  relative accuracy: 0.00  R2 score: 49.48\n",
      "INFO:root: Epoch 30: training loss = 0.06450485  validation loss = 0.06410784  learning rate = 2.003e-04  relative accuracy: 0.00  R2 score: 49.42\n",
      "INFO:root: Epoch 31: training loss = 0.06350432  validation loss = 0.06294410  learning rate = 1.895e-04  relative accuracy: 0.00  R2 score: 50.36\n",
      "INFO:root: Epoch 32: training loss = 0.06469059  validation loss = 0.06337941  learning rate = 1.792e-04  relative accuracy: 0.00  R2 score: 50.31\n",
      "INFO:root: Epoch 33: training loss = 0.06303827  validation loss = 0.06311673  learning rate = 1.696e-04  relative accuracy: 0.00  R2 score: 50.25\n",
      "INFO:root: Epoch 34: training loss = 0.06386779  validation loss = 0.06291260  learning rate = 1.604e-04  relative accuracy: 0.00  R2 score: 50.65\n",
      "INFO:root: Epoch 35: training loss = 0.06369625  validation loss = 0.06240922  learning rate = 1.518e-04  relative accuracy: 0.00  R2 score: 50.30\n",
      "INFO:root: Epoch 36: training loss = 0.06313323  validation loss = 0.06327466  learning rate = 1.436e-04  relative accuracy: 0.00  R2 score: 50.08\n",
      "INFO:root: Epoch 37: training loss = 0.06277906  validation loss = 0.06311051  learning rate = 1.358e-04  relative accuracy: 0.00  R2 score: 50.69\n",
      "INFO:root: Epoch 38: training loss = 0.06344429  validation loss = 0.06353108  learning rate = 1.285e-04  relative accuracy: 0.00  R2 score: 49.92\n",
      "INFO:root: Epoch 39: training loss = 0.06278280  validation loss = 0.06388945  learning rate = 1.216e-04  relative accuracy: 0.00  R2 score: 49.78\n",
      "INFO:root: Epoch 40: training loss = 0.06217667  validation loss = 0.06228465  learning rate = 1.150e-04  relative accuracy: 0.00  R2 score: 51.21\n",
      "INFO:root: Epoch 41: training loss = 0.06228314  validation loss = 0.06311783  learning rate = 1.088e-04  relative accuracy: 0.00  R2 score: 50.47\n",
      "INFO:root: Epoch 42: training loss = 0.06236087  validation loss = 0.06148184  learning rate = 1.029e-04  relative accuracy: 0.00  R2 score: 51.91\n",
      "INFO:root: Epoch 43: training loss = 0.06197386  validation loss = 0.06211064  learning rate = 9.740e-05  relative accuracy: 0.00  R2 score: 51.51\n",
      "INFO:root: Epoch 44: training loss = 0.06209379  validation loss = 0.06335887  learning rate = 9.214e-05  relative accuracy: 0.00  R2 score: 50.30\n",
      "INFO:root: Epoch 45: training loss = 0.06158698  validation loss = 0.06222518  learning rate = 8.717e-05  relative accuracy: 0.00  R2 score: 51.17\n",
      "INFO:root: Epoch 46: training loss = 0.06211465  validation loss = 0.06171112  learning rate = 8.247e-05  relative accuracy: 0.00  R2 score: 50.99\n",
      "INFO:root: Epoch 47: training loss = 0.06221195  validation loss = 0.06075818  learning rate = 7.802e-05  relative accuracy: 0.00  R2 score: 51.84\n",
      "INFO:root: Epoch 48: training loss = 0.06171815  validation loss = 0.06271096  learning rate = 7.381e-05  relative accuracy: 0.00  R2 score: 50.83\n",
      "INFO:root: Epoch 49: training loss = 0.06228257  validation loss = 0.06204807  learning rate = 6.983e-05  relative accuracy: 0.00  R2 score: 51.06\n",
      "INFO:root: Epoch 50: training loss = 0.06078462  validation loss = 0.06111742  learning rate = 6.606e-05  relative accuracy: 0.00  R2 score: 52.17\n",
      "INFO:root: Epoch 51: training loss = 0.06161547  validation loss = 0.06092676  learning rate = 6.250e-05  relative accuracy: 0.00  R2 score: 51.86\n",
      "INFO:root: Epoch 52: training loss = 0.06147335  validation loss = 0.06150810  learning rate = 5.913e-05  relative accuracy: 0.00  R2 score: 51.53\n",
      "INFO:root: Epoch 53: training loss = 0.06069050  validation loss = 0.06084861  learning rate = 5.594e-05  relative accuracy: 0.00  R2 score: 51.95\n",
      "INFO:root: Epoch 54: training loss = 0.06082953  validation loss = 0.06156307  learning rate = 5.292e-05  relative accuracy: 0.00  R2 score: 51.75\n",
      "INFO:root: Epoch 55: training loss = 0.06139575  validation loss = 0.06151831  learning rate = 5.007e-05  relative accuracy: 0.00  R2 score: 51.40\n",
      "INFO:root: Epoch 56: training loss = 0.06177579  validation loss = 0.06063484  learning rate = 4.737e-05  relative accuracy: 0.00  R2 score: 52.14\n",
      "INFO:root: Epoch 57: training loss = 0.06041413  validation loss = 0.06141818  learning rate = 4.481e-05  relative accuracy: 0.00  R2 score: 51.97\n",
      "INFO:root: Epoch 58: training loss = 0.06156637  validation loss = 0.06096826  learning rate = 4.239e-05  relative accuracy: 0.00  R2 score: 51.65\n",
      "INFO:root: Epoch 59: training loss = 0.06057117  validation loss = 0.06035261  learning rate = 4.011e-05  relative accuracy: 0.00  R2 score: 52.45\n",
      "INFO:root: Epoch 60: training loss = 0.06036338  validation loss = 0.06039216  learning rate = 3.794e-05  relative accuracy: 0.00  R2 score: 52.30\n",
      "INFO:root: Epoch 61: training loss = 0.06044717  validation loss = 0.06058824  learning rate = 3.590e-05  relative accuracy: 0.00  R2 score: 52.59\n",
      "INFO:root: Epoch 62: training loss = 0.06090690  validation loss = 0.06122424  learning rate = 3.396e-05  relative accuracy: 0.00  R2 score: 51.69\n",
      "INFO:root: Epoch 63: training loss = 0.06192390  validation loss = 0.05924961  learning rate = 3.213e-05  relative accuracy: 0.00  R2 score: 52.90\n",
      "INFO:root: Epoch 64: training loss = 0.06028217  validation loss = 0.06114429  learning rate = 3.040e-05  relative accuracy: 0.00  R2 score: 51.66\n",
      "INFO:root: Epoch 65: training loss = 0.06025966  validation loss = 0.06036844  learning rate = 2.876e-05  relative accuracy: 0.00  R2 score: 52.59\n",
      "INFO:root: Epoch 66: training loss = 0.06050847  validation loss = 0.06048131  learning rate = 2.720e-05  relative accuracy: 0.00  R2 score: 52.68\n",
      "INFO:root: Epoch 67: training loss = 0.06116113  validation loss = 0.06011731  learning rate = 2.574e-05  relative accuracy: 0.00  R2 score: 52.65\n",
      "INFO:root: Epoch 68: training loss = 0.06001894  validation loss = 0.06120389  learning rate = 2.435e-05  relative accuracy: 0.00  R2 score: 51.71\n",
      "INFO:root: Epoch 69: training loss = 0.06040241  validation loss = 0.06028554  learning rate = 2.304e-05  relative accuracy: 0.00  R2 score: 52.07\n",
      "INFO:root: Epoch 70: training loss = 0.06053705  validation loss = 0.06064828  learning rate = 2.179e-05  relative accuracy: 0.00  R2 score: 52.51\n",
      "INFO:root: Epoch 71: training loss = 0.06079009  validation loss = 0.06090702  learning rate = 2.062e-05  relative accuracy: 0.00  R2 score: 52.20\n",
      "INFO:root: Epoch 72: training loss = 0.06137633  validation loss = 0.06126076  learning rate = 1.951e-05  relative accuracy: 0.00  R2 score: 51.46\n",
      "INFO:root: Epoch 73: training loss = 0.06014444  validation loss = 0.06051524  learning rate = 1.845e-05  relative accuracy: 0.00  R2 score: 52.33\n",
      "INFO:root: Epoch 74: training loss = 0.06110468  validation loss = 0.06104714  learning rate = 1.746e-05  relative accuracy: 0.00  R2 score: 52.05\n",
      "INFO:root: Epoch 75: training loss = 0.05972582  validation loss = 0.06101185  learning rate = 1.652e-05  relative accuracy: 0.00  R2 score: 52.21\n",
      "INFO:root: Epoch 76: training loss = 0.05993845  validation loss = 0.06093405  learning rate = 1.562e-05  relative accuracy: 0.00  R2 score: 52.42\n",
      "INFO:root: Epoch 77: training loss = 0.06052593  validation loss = 0.05951111  learning rate = 1.478e-05  relative accuracy: 0.00  R2 score: 52.96\n",
      "INFO:root: Epoch 78: training loss = 0.06015844  validation loss = 0.05985130  learning rate = 1.398e-05  relative accuracy: 0.00  R2 score: 52.69\n",
      "INFO:root: Epoch 79: training loss = 0.06043428  validation loss = 0.06026284  learning rate = 1.323e-05  relative accuracy: 0.00  R2 score: 52.49\n",
      "INFO:root: Epoch 80: training loss = 0.06027880  validation loss = 0.06031650  learning rate = 1.252e-05  relative accuracy: 0.00  R2 score: 52.63\n",
      "INFO:root: Epoch 81: training loss = 0.05991481  validation loss = 0.06093086  learning rate = 1.184e-05  relative accuracy: 0.00  R2 score: 52.54\n",
      "INFO:root: Epoch 82: training loss = 0.05986007  validation loss = 0.06117985  learning rate = 1.120e-05  relative accuracy: 0.00  R2 score: 52.24\n",
      "INFO:root: Epoch 83: training loss = 0.06033173  validation loss = 0.06118361  learning rate = 1.060e-05  relative accuracy: 0.00  R2 score: 51.69\n",
      "INFO:root: Epoch 84: training loss = 0.05986978  validation loss = 0.05911191  learning rate = 1.003e-05  relative accuracy: 0.00  R2 score: 53.12\n",
      "INFO:root: Epoch 85: training loss = 0.05971660  validation loss = 0.05964345  learning rate = 9.486e-06  relative accuracy: 0.00  R2 score: 53.05\n",
      "INFO:root: Epoch 86: training loss = 0.05990154  validation loss = 0.06032625  learning rate = 8.974e-06  relative accuracy: 0.00  R2 score: 52.43\n",
      "INFO:root: Epoch 87: training loss = 0.05990326  validation loss = 0.06166382  learning rate = 8.490e-06  relative accuracy: 0.00  R2 score: 51.74\n",
      "INFO:root: Epoch 88: training loss = 0.05968506  validation loss = 0.06058012  learning rate = 8.032e-06  relative accuracy: 0.00  R2 score: 53.09\n",
      "INFO:root: Epoch 89: training loss = 0.06097279  validation loss = 0.06008954  learning rate = 7.599e-06  relative accuracy: 0.00  R2 score: 52.42\n",
      "INFO:root: Epoch 90: training loss = 0.06060817  validation loss = 0.06028086  learning rate = 7.189e-06  relative accuracy: 0.00  R2 score: 52.52\n",
      "INFO:root: Epoch 91: training loss = 0.06053602  validation loss = 0.05935000  learning rate = 6.801e-06  relative accuracy: 0.00  R2 score: 52.95\n",
      "INFO:root: Epoch 92: training loss = 0.06016564  validation loss = 0.05977974  learning rate = 6.434e-06  relative accuracy: 0.00  R2 score: 52.65\n",
      "INFO:root: Epoch 93: training loss = 0.06078132  validation loss = 0.06095277  learning rate = 6.087e-06  relative accuracy: 0.00  R2 score: 52.27\n",
      "INFO:root: Epoch 94: training loss = 0.06041862  validation loss = 0.05991790  learning rate = 5.759e-06  relative accuracy: 0.00  R2 score: 53.18\n",
      "INFO:root: Epoch 95: training loss = 0.06090805  validation loss = 0.06050085  learning rate = 5.448e-06  relative accuracy: 0.00  R2 score: 52.69\n",
      "INFO:root: Epoch 96: training loss = 0.06027410  validation loss = 0.06043832  learning rate = 5.154e-06  relative accuracy: 0.00  R2 score: 52.32\n",
      "INFO:root: Epoch 97: training loss = 0.06024493  validation loss = 0.06072505  learning rate = 4.876e-06  relative accuracy: 0.00  R2 score: 51.88\n",
      "INFO:root: Epoch 98: training loss = 0.06030483  validation loss = 0.06013029  learning rate = 4.613e-06  relative accuracy: 0.00  R2 score: 52.80\n",
      "INFO:root: Epoch 99: training loss = 0.05914668  validation loss = 0.05972448  learning rate = 4.364e-06  relative accuracy: 0.00  R2 score: 52.59\n",
      "INFO:root: Epoch 100: training loss = 0.06099197  validation loss = 0.06171824  learning rate = 4.129e-06  relative accuracy: 0.00  R2 score: 52.32\n",
      "INFO:root: Epoch 101: training loss = 0.05959791  validation loss = 0.06108593  learning rate = 3.906e-06  relative accuracy: 0.00  R2 score: 52.17\n",
      "INFO:root: Epoch 102: training loss = 0.05981109  validation loss = 0.06149443  learning rate = 3.696e-06  relative accuracy: 0.00  R2 score: 51.62\n",
      "INFO:root: Epoch 103: training loss = 0.06013918  validation loss = 0.06078267  learning rate = 3.496e-06  relative accuracy: 0.00  R2 score: 52.62\n",
      "INFO:root: Epoch 104: training loss = 0.05999665  validation loss = 0.05966920  learning rate = 3.308e-06  relative accuracy: 0.00  R2 score: 53.01\n",
      "INFO:root: Epoch 105: training loss = 0.05976337  validation loss = 0.06028773  learning rate = 3.129e-06  relative accuracy: 0.00  R2 score: 52.22\n",
      "INFO:root: Epoch 106: training loss = 0.05990555  validation loss = 0.06004072  learning rate = 2.960e-06  relative accuracy: 0.00  R2 score: 52.60\n",
      "INFO:root: Epoch 107: training loss = 0.06051376  validation loss = 0.06040686  learning rate = 2.801e-06  relative accuracy: 0.00  R2 score: 52.61\n",
      "INFO:root: Epoch 108: training loss = 0.05998679  validation loss = 0.06075503  learning rate = 2.650e-06  relative accuracy: 0.00  R2 score: 52.63\n",
      "INFO:root: Epoch 109: training loss = 0.06037410  validation loss = 0.06016007  learning rate = 2.507e-06  relative accuracy: 0.00  R2 score: 52.44\n",
      "INFO:root: Epoch 110: training loss = 0.06002882  validation loss = 0.05935046  learning rate = 2.371e-06  relative accuracy: 0.00  R2 score: 52.98\n",
      "INFO:root: Epoch 111: training loss = 0.06087514  validation loss = 0.05990788  learning rate = 2.244e-06  relative accuracy: 0.00  R2 score: 52.92\n",
      "INFO:root: Epoch 112: training loss = 0.06015634  validation loss = 0.05956899  learning rate = 2.123e-06  relative accuracy: 0.00  R2 score: 52.78\n",
      "INFO:root: Epoch 113: training loss = 0.06026508  validation loss = 0.06077671  learning rate = 2.008e-06  relative accuracy: 0.00  R2 score: 52.29\n",
      "INFO:root: Epoch 114: training loss = 0.06062017  validation loss = 0.06021418  learning rate = 1.900e-06  relative accuracy: 0.00  R2 score: 52.47\n",
      "INFO:root: Epoch 115: training loss = 0.05931956  validation loss = 0.06065716  learning rate = 1.797e-06  relative accuracy: 0.00  R2 score: 52.64\n",
      "INFO:root: Epoch 116: training loss = 0.05951417  validation loss = 0.05998348  learning rate = 1.700e-06  relative accuracy: 0.00  R2 score: 52.22\n",
      "INFO:root: Epoch 117: training loss = 0.05979636  validation loss = 0.06028667  learning rate = 1.609e-06  relative accuracy: 0.00  R2 score: 52.62\n",
      "INFO:root: Epoch 118: training loss = 0.05978923  validation loss = 0.06010589  learning rate = 1.522e-06  relative accuracy: 0.00  R2 score: 52.83\n",
      "INFO:root: Epoch 119: training loss = 0.06045990  validation loss = 0.05933042  learning rate = 1.440e-06  relative accuracy: 0.00  R2 score: 52.90\n",
      "INFO:root: Epoch 120: training loss = 0.06045948  validation loss = 0.05906694  learning rate = 1.362e-06  relative accuracy: 0.00  R2 score: 53.51\n",
      "INFO:root: Epoch 121: training loss = 0.05972025  validation loss = 0.06022308  learning rate = 1.289e-06  relative accuracy: 0.00  R2 score: 52.84\n",
      "INFO:root: Epoch 122: training loss = 0.05975373  validation loss = 0.05955881  learning rate = 1.219e-06  relative accuracy: 0.00  R2 score: 53.21\n",
      "INFO:root: Epoch 123: training loss = 0.05994612  validation loss = 0.06040859  learning rate = 1.153e-06  relative accuracy: 0.00  R2 score: 52.65\n",
      "INFO:root: Epoch 124: training loss = 0.05996305  validation loss = 0.05999772  learning rate = 1.091e-06  relative accuracy: 0.00  R2 score: 52.33\n",
      "INFO:root: Epoch 125: training loss = 0.06027684  validation loss = 0.05983595  learning rate = 1.032e-06  relative accuracy: 0.00  R2 score: 52.53\n",
      "INFO:root: Epoch 126: training loss = 0.06018310  validation loss = 0.05953358  learning rate = 9.766e-07  relative accuracy: 0.00  R2 score: 53.39\n",
      "INFO:root: Epoch 127: training loss = 0.05988952  validation loss = 0.05991003  learning rate = 9.239e-07  relative accuracy: 0.00  R2 score: 52.53\n",
      "INFO:root: Epoch 128: training loss = 0.05998495  validation loss = 0.06081101  learning rate = 8.740e-07  relative accuracy: 0.00  R2 score: 52.47\n",
      "INFO:root: Epoch 129: training loss = 0.06056484  validation loss = 0.06045968  learning rate = 8.269e-07  relative accuracy: 0.00  R2 score: 52.75\n",
      "INFO:root: Epoch 130: training loss = 0.05966365  validation loss = 0.05929821  learning rate = 7.823e-07  relative accuracy: 0.00  R2 score: 53.26\n",
      "INFO:root: Epoch 131: training loss = 0.06098244  validation loss = 0.05967280  learning rate = 7.401e-07  relative accuracy: 0.00  R2 score: 53.33\n",
      "INFO:root: Epoch 132: training loss = 0.05975065  validation loss = 0.05977012  learning rate = 7.002e-07  relative accuracy: 0.00  R2 score: 52.96\n",
      "INFO:root: Epoch 133: training loss = 0.06086839  validation loss = 0.06089153  learning rate = 6.624e-07  relative accuracy: 0.00  R2 score: 52.65\n",
      "INFO:root: Epoch 134: training loss = 0.05913111  validation loss = 0.06002361  learning rate = 6.267e-07  relative accuracy: 0.00  R2 score: 52.45\n",
      "INFO:root: Epoch 135: training loss = 0.06027576  validation loss = 0.06027786  learning rate = 5.929e-07  relative accuracy: 0.00  R2 score: 52.51\n",
      "INFO:root: Epoch 136: training loss = 0.05999551  validation loss = 0.05981165  learning rate = 5.609e-07  relative accuracy: 0.00  R2 score: 52.25\n",
      "INFO:root: Epoch 137: training loss = 0.06009985  validation loss = 0.05949407  learning rate = 5.306e-07  relative accuracy: 0.00  R2 score: 52.82\n",
      "INFO:root: Epoch 138: training loss = 0.06026745  validation loss = 0.05963111  learning rate = 5.020e-07  relative accuracy: 0.00  R2 score: 53.35\n",
      "INFO:root: Epoch 139: training loss = 0.05977472  validation loss = 0.06072568  learning rate = 4.749e-07  relative accuracy: 0.00  R2 score: 52.09\n",
      "INFO:root: Epoch 140: training loss = 0.05974358  validation loss = 0.06035278  learning rate = 4.493e-07  relative accuracy: 0.00  R2 score: 52.08\n",
      "INFO:root: Epoch 141: training loss = 0.06031630  validation loss = 0.05993170  learning rate = 4.251e-07  relative accuracy: 0.00  R2 score: 52.30\n",
      "INFO:root: Epoch 142: training loss = 0.06039980  validation loss = 0.06029736  learning rate = 4.021e-07  relative accuracy: 0.00  R2 score: 52.21\n",
      "INFO:root: Epoch 143: training loss = 0.06005318  validation loss = 0.05970914  learning rate = 3.805e-07  relative accuracy: 0.00  R2 score: 52.77\n",
      "INFO:root: Epoch 144: training loss = 0.05976900  validation loss = 0.06004210  learning rate = 3.599e-07  relative accuracy: 0.00  R2 score: 52.89\n",
      "INFO:root: Epoch 145: training loss = 0.06035302  validation loss = 0.05921572  learning rate = 3.405e-07  relative accuracy: 0.00  R2 score: 52.80\n",
      "INFO:root: Epoch 146: training loss = 0.05955600  validation loss = 0.05978458  learning rate = 3.221e-07  relative accuracy: 0.00  R2 score: 53.18\n",
      "INFO:root: Epoch 147: training loss = 0.06010213  validation loss = 0.05975666  learning rate = 3.048e-07  relative accuracy: 0.00  R2 score: 52.41\n",
      "INFO:root: Epoch 148: training loss = 0.05979603  validation loss = 0.06025688  learning rate = 2.883e-07  relative accuracy: 0.00  R2 score: 52.43\n",
      "INFO:root: Epoch 149: training loss = 0.06071355  validation loss = 0.06029703  learning rate = 2.728e-07  relative accuracy: 0.00  R2 score: 52.07\n",
      "INFO:root: Epoch 150: training loss = 0.05994691  validation loss = 0.06010110  learning rate = 2.581e-07  relative accuracy: 0.00  R2 score: 52.50\n",
      "INFO:root: Epoch 151: training loss = 0.05982246  validation loss = 0.06010572  learning rate = 2.441e-07  relative accuracy: 0.00  R2 score: 52.91\n",
      "INFO:root: Epoch 152: training loss = 0.05993300  validation loss = 0.06028276  learning rate = 2.310e-07  relative accuracy: 0.00  R2 score: 52.57\n",
      "INFO:root: Epoch 153: training loss = 0.05953683  validation loss = 0.06018918  learning rate = 2.185e-07  relative accuracy: 0.00  R2 score: 52.80\n",
      "INFO:root: Epoch 154: training loss = 0.05993522  validation loss = 0.06037495  learning rate = 2.067e-07  relative accuracy: 0.00  R2 score: 52.78\n",
      "INFO:root: Epoch 155: training loss = 0.05941024  validation loss = 0.06017176  learning rate = 1.956e-07  relative accuracy: 0.00  R2 score: 52.32\n",
      "INFO:root: Epoch 156: training loss = 0.05958238  validation loss = 0.06042940  learning rate = 1.850e-07  relative accuracy: 0.00  R2 score: 52.17\n",
      "INFO:root: Epoch 157: training loss = 0.06016881  validation loss = 0.06002783  learning rate = 1.750e-07  relative accuracy: 0.00  R2 score: 53.11\n",
      "INFO:root: Epoch 158: training loss = 0.05985142  validation loss = 0.06038478  learning rate = 1.656e-07  relative accuracy: 0.00  R2 score: 52.63\n",
      "INFO:root: Epoch 159: training loss = 0.06059679  validation loss = 0.06075601  learning rate = 1.567e-07  relative accuracy: 0.00  R2 score: 52.46\n",
      "INFO:root: Epoch 160: training loss = 0.05958159  validation loss = 0.06038297  learning rate = 1.482e-07  relative accuracy: 0.00  R2 score: 52.38\n",
      "INFO:root: Epoch 161: training loss = 0.05933404  validation loss = 0.05935073  learning rate = 1.402e-07  relative accuracy: 0.00  R2 score: 52.87\n",
      "INFO:root: Epoch 162: training loss = 0.06042873  validation loss = 0.06048823  learning rate = 1.327e-07  relative accuracy: 0.00  R2 score: 52.21\n",
      "INFO:root: Epoch 163: training loss = 0.06000563  validation loss = 0.05996891  learning rate = 1.255e-07  relative accuracy: 0.00  R2 score: 52.86\n",
      "INFO:root: Epoch 164: training loss = 0.06032612  validation loss = 0.06081148  learning rate = 1.187e-07  relative accuracy: 0.00  R2 score: 51.93\n",
      "INFO:root: Epoch 165: training loss = 0.06083997  validation loss = 0.05949853  learning rate = 1.123e-07  relative accuracy: 0.00  R2 score: 52.89\n",
      "INFO:root: Epoch 166: training loss = 0.05974228  validation loss = 0.06051899  learning rate = 1.063e-07  relative accuracy: 0.00  R2 score: 52.46\n",
      "INFO:root: Epoch 167: training loss = 0.05991098  validation loss = 0.05994442  learning rate = 1.005e-07  relative accuracy: 0.00  R2 score: 52.65\n",
      "INFO:root: Epoch 168: training loss = 0.05961388  validation loss = 0.06054413  learning rate = 9.511e-08  relative accuracy: 0.00  R2 score: 52.47\n",
      "INFO:root: Epoch 169: training loss = 0.05933964  validation loss = 0.05966859  learning rate = 8.998e-08  relative accuracy: 0.00  R2 score: 53.52\n",
      "INFO:root: Epoch 170: training loss = 0.06048864  validation loss = 0.05971611  learning rate = 8.513e-08  relative accuracy: 0.00  R2 score: 52.96\n",
      "INFO:root: running post-process\n",
      "INFO:root: running the DNN predictions and accuracy computation\n",
      "INFO:root: relative accuracy: 0.00%  |---|  R2 score: 52.46%\n",
      "INFO:root: printing training evaluation plots\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbp",
   "language": "python",
   "name": "rbp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
